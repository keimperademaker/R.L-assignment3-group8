{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "/Users/keimperademaker/opt/anaconda3/envs/prins-assignment/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 100/10000] Length=0.40 | Eps=0.606 | AvgReward(100)=18.6\n",
      "[Episode 200/10000] Length=0.40 | Eps=0.367 | AvgReward(100)=29.4\n",
      "[Episode 300/10000] Length=0.40 | Eps=0.222 | AvgReward(100)=104.1\n",
      "[Episode 400/10000] Length=0.40 | Eps=0.135 | AvgReward(100)=92.8\n",
      "[Episode 500/10000] Length=0.40 | Eps=0.082 | AvgReward(100)=89.9\n",
      "[Episode 600/10000] Length=0.40 | Eps=0.049 | AvgReward(100)=118.6\n",
      "[Episode 700/10000] Length=0.40 | Eps=0.030 | AvgReward(100)=136.6\n",
      "[Episode 800/10000] Length=0.40 | Eps=0.018 | AvgReward(100)=541.3\n",
      "[Episode 900/10000] Length=0.40 | Eps=0.011 | AvgReward(100)=807.7\n",
      "[Episode 1000/10000] Length=0.40 | Eps=0.010 | AvgReward(100)=686.3\n",
      "[Episode 1100/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=113.5\n",
      "[Episode 1200/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=189.4\n",
      "[Episode 1300/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=207.1\n",
      "[Episode 1400/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=992.8\n",
      "[Episode 1500/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=464.9\n",
      "[Episode 1600/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=927.1\n",
      "[Episode 1700/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=689.7\n",
      "[Episode 1800/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=618.0\n",
      "[Episode 1900/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=870.3\n",
      "[Episode 2000/10000] Length=0.56 | Eps=0.010 | AvgReward(100)=715.0\n",
      "[Episode 2100/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=102.8\n",
      "[Episode 2200/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=763.8\n",
      "[Episode 2300/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=255.8\n",
      "[Episode 2400/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=1250.3\n",
      "[Episode 2500/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=207.9\n",
      "[Episode 2600/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=814.8\n",
      "[Episode 2700/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=650.0\n",
      "[Episode 2800/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=522.0\n",
      "[Episode 2900/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=675.2\n",
      "[Episode 3000/10000] Length=0.71 | Eps=0.010 | AvgReward(100)=387.4\n",
      "[Episode 3100/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=211.6\n",
      "[Episode 3200/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=582.4\n",
      "[Episode 3300/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=1100.6\n",
      "[Episode 3400/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=691.5\n",
      "[Episode 3500/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=1382.0\n",
      "[Episode 3600/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=418.3\n",
      "[Episode 3700/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=372.0\n",
      "[Episode 3800/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=348.6\n",
      "[Episode 3900/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=321.8\n",
      "[Episode 4000/10000] Length=0.87 | Eps=0.010 | AvgReward(100)=245.5\n",
      "[Episode 4100/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=456.7\n",
      "[Episode 4200/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=971.7\n",
      "[Episode 4300/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=577.5\n",
      "[Episode 4400/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=238.6\n",
      "[Episode 4500/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=1482.2\n",
      "[Episode 4600/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=2652.8\n",
      "[Episode 4700/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=156.0\n",
      "[Episode 4800/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=625.0\n",
      "[Episode 4900/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=323.2\n",
      "[Episode 5000/10000] Length=1.02 | Eps=0.010 | AvgReward(100)=105.5\n",
      "[Episode 5100/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=110.2\n",
      "[Episode 5200/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=709.7\n",
      "[Episode 5300/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=255.2\n",
      "[Episode 5400/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=168.3\n",
      "[Episode 5500/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=372.7\n",
      "[Episode 5600/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=630.1\n",
      "[Episode 5700/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=128.6\n",
      "[Episode 5800/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=131.7\n",
      "[Episode 5900/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=1157.5\n",
      "[Episode 6000/10000] Length=1.18 | Eps=0.010 | AvgReward(100)=1505.8\n",
      "[Episode 6100/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=617.9\n",
      "[Episode 6200/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=100.2\n",
      "[Episode 6300/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=125.1\n",
      "[Episode 6400/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=536.3\n",
      "[Episode 6500/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=104.0\n",
      "[Episode 6600/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=117.0\n",
      "[Episode 6700/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=115.9\n",
      "[Episode 6800/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=123.9\n",
      "[Episode 6900/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=493.1\n",
      "[Episode 7000/10000] Length=1.33 | Eps=0.010 | AvgReward(100)=1463.9\n",
      "[Episode 7100/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=155.8\n",
      "[Episode 7200/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=106.0\n",
      "[Episode 7300/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=201.2\n",
      "[Episode 7400/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=582.8\n",
      "[Episode 7500/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=103.7\n",
      "[Episode 7600/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=101.0\n",
      "[Episode 7700/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=121.8\n",
      "[Episode 7800/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=159.0\n",
      "[Episode 7900/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=126.0\n",
      "[Episode 8000/10000] Length=1.49 | Eps=0.010 | AvgReward(100)=212.6\n",
      "[Episode 8100/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=733.4\n",
      "[Episode 8200/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=104.4\n",
      "[Episode 8300/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=111.8\n",
      "[Episode 8400/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=111.3\n",
      "[Episode 8500/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=111.0\n",
      "[Episode 8600/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=96.5\n",
      "[Episode 8700/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=157.0\n",
      "[Episode 8800/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=128.2\n",
      "[Episode 8900/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=152.0\n",
      "[Episode 9000/10000] Length=1.64 | Eps=0.010 | AvgReward(100)=170.1\n",
      "[Episode 9100/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=395.5\n",
      "[Episode 9200/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=1464.6\n",
      "[Episode 9300/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=606.8\n",
      "[Episode 9400/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=137.5\n",
      "[Episode 9500/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=168.1\n",
      "[Episode 9600/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=2216.0\n",
      "[Episode 9700/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=89.7\n",
      "[Episode 9800/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=141.1\n",
      "[Episode 9900/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=1387.4\n",
      "[Episode 10000/10000] Length=1.80 | Eps=0.010 | AvgReward(100)=218.0\n",
      "Training complete. Model saved to weights/dqn_curriculum_short_to_long.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "#  Q-Network (standard network)\n",
    "# ===============================\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Replay Buffer\n",
    "# ===============================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
    "        return (\n",
    "            torch.tensor(state, dtype=torch.float32),\n",
    "            torch.tensor(action, dtype=torch.int64),\n",
    "            torch.tensor(reward, dtype=torch.float32),\n",
    "            torch.tensor(next_state, dtype=torch.float32),\n",
    "            torch.tensor(done, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Epsilon-greedy policy\n",
    "# ===============================\n",
    "def select_action(q_network, state, epsilon, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_dim - 1)\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "    return q_values.argmax().item()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Train Step\n",
    "# ===============================\n",
    "def train_step(q_network, target_network, optimizer, replay_buffer, batch_size, gamma=0.99):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return 0.0\n",
    "\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Compute Q targets\n",
    "    with torch.no_grad():\n",
    "        target_q = rewards + gamma * (1 - dones) * target_network(next_states).max(1)[0]\n",
    "\n",
    "    current_q = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Main Curriculum Training Loop\n",
    "# ===============================\n",
    "def train_curriculum_short_to_long(\n",
    "    total_episodes=10000,\n",
    "    num_bins=10,\n",
    "    buffer_capacity=100000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    target_update_freq=1000,\n",
    "    render=False,\n",
    "    save_path=\"weights/dqn_curriculum_short_to_long.pth\"\n",
    "):\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    target_network = QNetwork(state_dim, action_dim)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "    # Curriculum setup\n",
    "    lengths = np.linspace(0.4, 1.8, num_bins)\n",
    "    episodes_per_bin = total_episodes // num_bins\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    all_rewards = []\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        # Determine current bin (progress linearly)\n",
    "        bin_idx = min(episode // episodes_per_bin, num_bins - 1)\n",
    "        current_length = lengths[bin_idx]\n",
    "\n",
    "        env.unwrapped.length = current_length\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(q_network, state, epsilon, action_dim)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            loss = train_step(q_network, target_network, optimizer, replay_buffer, batch_size, gamma)\n",
    "\n",
    "            total_steps += 1\n",
    "            if total_steps % target_update_freq == 0:\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_end)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(all_rewards[-100:])\n",
    "            print(f\"[Episode {episode+1}/{total_episodes}] \"\n",
    "                  f\"Length={current_length:.2f} | Eps={epsilon:.3f} | \"\n",
    "                  f\"AvgReward(100)={avg_reward:.1f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(q_network.state_dict(), save_path)\n",
    "    print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Entry Point\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    train_curriculum_short_to_long(\n",
    "        total_episodes=10000,\n",
    "        num_bins=10,\n",
    "        buffer_capacity=100000,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        target_update_freq=1000,\n",
    "        render=False,\n",
    "        save_path=\"weights/dqn_curriculum_short_to_long.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prins-assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
