{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 100/1000] Length=1.80 | Eps=0.606 | AvgReward(100)=54.4\n",
      "[Episode 200/1000] Length=1.64 | Eps=0.367 | AvgReward(100)=108.4\n",
      "[Episode 300/1000] Length=1.49 | Eps=0.222 | AvgReward(100)=116.2\n",
      "[Episode 400/1000] Length=1.33 | Eps=0.135 | AvgReward(100)=109.2\n",
      "[Episode 500/1000] Length=1.18 | Eps=0.082 | AvgReward(100)=423.1\n",
      "[Episode 600/1000] Length=1.02 | Eps=0.049 | AvgReward(100)=1142.1\n",
      "[Episode 700/1000] Length=0.87 | Eps=0.030 | AvgReward(100)=817.7\n",
      "[Episode 800/1000] Length=0.71 | Eps=0.018 | AvgReward(100)=732.7\n",
      "[Episode 900/1000] Length=0.56 | Eps=0.011 | AvgReward(100)=224.0\n",
      "[Episode 1000/1000] Length=0.40 | Eps=0.010 | AvgReward(100)=1046.4\n",
      "Training complete. Model saved to weights/dqn_curriculum_short_to_long.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "#  Q-Network (standard network)\n",
    "# ===============================\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Replay Buffer\n",
    "# ===============================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
    "        return (\n",
    "            torch.tensor(state, dtype=torch.float32),\n",
    "            torch.tensor(action, dtype=torch.int64),\n",
    "            torch.tensor(reward, dtype=torch.float32),\n",
    "            torch.tensor(next_state, dtype=torch.float32),\n",
    "            torch.tensor(done, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Epsilon-greedy policy\n",
    "# ===============================\n",
    "def select_action(q_network, state, epsilon, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_dim - 1)\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "    return q_values.argmax().item()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Train Step\n",
    "# ===============================\n",
    "def train_step(q_network, target_network, optimizer, replay_buffer, batch_size, gamma=0.99):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return 0.0\n",
    "\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Compute Q targets\n",
    "    with torch.no_grad():\n",
    "        target_q = rewards + gamma * (1 - dones) * target_network(next_states).max(1)[0]\n",
    "\n",
    "    current_q = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Main Curriculum Training Loop\n",
    "# ===============================\n",
    "def train_curriculum_long_to_short(\n",
    "    total_episodes=1000,\n",
    "    num_bins=10,\n",
    "    buffer_capacity=100000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    target_update_freq=1000,\n",
    "    render=False,\n",
    "    save_path=\"weights/dqn_curriculum_long_to_short.pth\"\n",
    "):\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    target_network = QNetwork(state_dim, action_dim)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "    # Curriculum setup\n",
    "    lengths = np.linspace(1.8, 0.4, num_bins)\n",
    "    episodes_per_bin = total_episodes // num_bins\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    all_rewards = []\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        # Determine current bin (progress linearly)\n",
    "        bin_idx = min(episode // episodes_per_bin, num_bins - 1)\n",
    "        current_length = lengths[bin_idx]\n",
    "\n",
    "        env.unwrapped.length = current_length\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = select_action(q_network, state, epsilon, action_dim)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "\n",
    "            loss = train_step(q_network, target_network, optimizer, replay_buffer, batch_size, gamma)\n",
    "\n",
    "            total_steps += 1\n",
    "            if total_steps % target_update_freq == 0:\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_end)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(all_rewards[-100:])\n",
    "            print(f\"[Episode {episode+1}/{total_episodes}] \"\n",
    "                  f\"Length={current_length:.2f} | Eps={epsilon:.3f} | \"\n",
    "                  f\"AvgReward(100)={avg_reward:.1f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(q_network.state_dict(), save_path)\n",
    "    print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Entry Point\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    train_curriculum_long_to_short(\n",
    "        total_episodes=1000,\n",
    "        num_bins=10,\n",
    "        buffer_capacity=100000,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        target_update_freq=1000,\n",
    "        render=False,\n",
    "        save_path=\"weights/dqn_curriculum_short_to_long.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
