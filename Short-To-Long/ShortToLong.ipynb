{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "/Users/keimperademaker/opt/anaconda3/envs/prins-assignment/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Episode 100/1000] Length=0.40 | Eps=0.606 | AvgReward(100)=17.4\n",
      "[Episode 200/1000] Length=0.56 | Eps=0.367 | AvgReward(100)=35.7\n",
      "[Episode 300/1000] Length=0.71 | Eps=0.222 | AvgReward(100)=102.8\n",
      "[Episode 400/1000] Length=0.87 | Eps=0.135 | AvgReward(100)=105.7\n",
      "[Episode 500/1000] Length=1.02 | Eps=0.082 | AvgReward(100)=106.4\n",
      "[Episode 600/1000] Length=1.18 | Eps=0.049 | AvgReward(100)=148.5\n",
      "[Episode 700/1000] Length=1.33 | Eps=0.030 | AvgReward(100)=148.0\n",
      "[Episode 800/1000] Length=1.49 | Eps=0.018 | AvgReward(100)=145.8\n",
      "[Episode 900/1000] Length=1.64 | Eps=0.011 | AvgReward(100)=155.7\n",
      "[Episode 1000/1000] Length=1.80 | Eps=0.010 | AvgReward(100)=175.9\n",
      "Training complete. Model saved to weights/dqn_curriculum_short_to_long.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from collections import deque\n",
    "import os\n",
    "\n",
    "# ===============================\n",
    "#  Q-Network (standard network)\n",
    "# ===============================\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Replay Buffer\n",
    "# ===============================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Sample a batch of experiences\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.array, zip(*batch))\n",
    "        return (\n",
    "            torch.tensor(state, dtype=torch.float32),\n",
    "            torch.tensor(action, dtype=torch.int64),\n",
    "            torch.tensor(reward, dtype=torch.float32),\n",
    "            torch.tensor(next_state, dtype=torch.float32),\n",
    "            torch.tensor(done, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Epsilon-greedy policy\n",
    "# ===============================\n",
    "def select_action(q_network, state, epsilon, action_dim):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_dim - 1)\n",
    "    with torch.no_grad():\n",
    "        q_values = q_network(torch.tensor(state, dtype=torch.float32).unsqueeze(0))\n",
    "    return q_values.argmax().item()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Train Step\n",
    "# ===============================\n",
    "def train_step(q_network, target_network, optimizer, replay_buffer, batch_size, gamma=0.99):\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        return 0.0\n",
    "\n",
    "    states, actions, rewards, next_states, dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Compute Q targets\n",
    "    with torch.no_grad():\n",
    "        target_q = rewards + gamma * (1 - dones) * target_network(next_states).max(1)[0]\n",
    "\n",
    "    current_q = q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Main Curriculum Training Loop\n",
    "# ===============================\n",
    "def train_curriculum_short_to_long(\n",
    "    \n",
    "    # Hyperparameters\n",
    "    total_episodes=10000,\n",
    "    num_bins=10,\n",
    "    buffer_capacity=100000,\n",
    "    batch_size=64,\n",
    "    gamma=0.99,\n",
    "    lr=1e-3,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=0.995,\n",
    "    target_update_freq=1000,\n",
    "    render=False,\n",
    "    save_path=\"weights/dqn_curriculum_short_to_long.pth\"\n",
    "):\n",
    "\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\" if render else None)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "\n",
    "    # Initialize networks and optimizer\n",
    "    q_network = QNetwork(state_dim, action_dim)\n",
    "    target_network = QNetwork(state_dim, action_dim)\n",
    "    target_network.load_state_dict(q_network.state_dict())\n",
    "    optimizer = optim.Adam(q_network.parameters(), lr=lr)\n",
    "    replay_buffer = ReplayBuffer(capacity=buffer_capacity)\n",
    "\n",
    "    # Curriculum setup\n",
    "    lengths = np.linspace(0.4, 1.8, num_bins)\n",
    "    episodes_per_bin = total_episodes // num_bins\n",
    "    epsilon = epsilon_start\n",
    "\n",
    "    all_rewards = []\n",
    "    total_steps = 0\n",
    "\n",
    "    for episode in range(total_episodes):\n",
    "        # Determine current bin (progress linearly)\n",
    "        bin_idx = min(episode // episodes_per_bin, num_bins - 1)\n",
    "        current_length = lengths[bin_idx]\n",
    "\n",
    "        env.unwrapped.length = current_length\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "\n",
    "        # Run episode\n",
    "        while not done:\n",
    "            action = select_action(q_network, state, epsilon, action_dim)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            replay_buffer.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            loss = train_step(q_network, target_network, optimizer, replay_buffer, batch_size, gamma)\n",
    "\n",
    "            total_steps += 1\n",
    "            if total_steps % target_update_freq == 0:\n",
    "                target_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon * epsilon_decay, epsilon_end)\n",
    "        all_rewards.append(episode_reward)\n",
    "\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            avg_reward = np.mean(all_rewards[-100:])\n",
    "            print(f\"[Episode {episode+1}/{total_episodes}] \"\n",
    "                  f\"Length={current_length:.2f} | Eps={epsilon:.3f} | \"\n",
    "                  f\"AvgReward(100)={avg_reward:.1f}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    torch.save(q_network.state_dict(), save_path)\n",
    "    print(f\"Training complete. Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "# ===============================\n",
    "#  Entry Point\n",
    "# ===============================\n",
    "if __name__ == \"__main__\":\n",
    "    train_curriculum_short_to_long(\n",
    "        total_episodes=1000,\n",
    "        num_bins=10,\n",
    "        buffer_capacity=100000,\n",
    "        batch_size=64,\n",
    "        gamma=0.99,\n",
    "        lr=1e-3,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        target_update_freq=1000,\n",
    "        render=False,\n",
    "        save_path=\"weights/dqn_curriculum_short_to_long.pth\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prins-assignment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
